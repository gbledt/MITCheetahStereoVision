%!TEX root = ../egpaper_final.tex
\section{Approach}

Our pipeline is summarized in Figure \ref{overview-diagram}. We begin with a pair of RGB stereo images. From this image pair and calibration parameters obtained offline, we apply a stereo matching algorithm (See Section \ref{stereo-matching-section} to get a depthmap of the scene. Using camera instrinsic parameters, we can project this depthmap into 3D space and obtain a point cloud of the environment (See Section \ref{pointcloud-section}). Using two different methods described in Section \ref{plane-section}, we decompose this point coud into a set of planes. Finally, we reconstruct a polytope from these planes. A robot can use this geometric representation of the terrain to plan its foot trajectory.


\begin{figure}[!h]
\centering
\includegraphics[width=2in]{Sections/Figures/Final-Project-Pipeline.png}
\caption{An overview of our obstacle reconstruction pipeline.}
\label{overview-diagram}
\end{figure}

\subsection{Stereo Matching for 3D Pointcloud} \label{stereo-matching-section}
The goal for stereo vision is to take two separate pictures of the same scene from two cameras at known relative distances and orientations form each other and attempt to reconstruct the 3D version of the scene. This is the general principle behind the human vision system that has depth perception. In our experiment we set two parallel cameras about 8 cm apart in the same direction which results in pictures like the one in Figure \ref{stereo-image-pair}.
\begin{figure}[!h]
\centering
\includegraphics[width=3.3in]{Sections/Figures/example_stereo_pair.jpg}
\caption{Am image pair taken by our stereo camera setup. The two cameras are parallel and have a baseline of approximately 8cm.}
\label{stereo-image-pair}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=2.3in]{Sections/Figures/stereo_anaglyph.png}
\caption{An example stereo anaglyph.}
\label{stereo-anaglyph}
\end{figure}

In order to find the best match between the two images, we used a technique called Semi-Global Matching as described in \cite{1467526}. The basic premise follows an optimization that compares the two images with a matching cost defined by the following cost function
\begin{align*}
E(D) = \sum\limits_p(C(p,D_p)+\sum\limits_{q\in N_p}P_1T[|D_p-D_q|=1]+\\\sum\limits_{q\in N_p}P_2T[|D_p-D_q|>1])
\end{align*}
Here, the optimization travels along many paths throughout the image and the costs from each path are summed and the lowest cost disparity is chosen for each pixel. The cost function has the first term being the sum of the pixel matching costs for the disparities, the second term being a penalty for 1 pixel disparity, and the third being a large cost for bigger pixel disparities.

The method results in a disparity image that gives the estimated depth to each pixel. This disparity is then used to create a 3D representation of the scene since we know the depth at each 2D pixel. This results in a pointcloud with $Z$ being the distance from the camera locations. The original image in then overlaid onto the pointcloud as seen in Figure \ref{pointcloud-example}. Although the pointcloud is not an exact representation of the actual scene, many post-processing steps were taken to smooth out the noise and extract features from the pointcloud.
\begin{figure}[!h]
\centering
\includegraphics[width=3.3in]{Sections/Figures/example_stairs_pointcloud.jpg}
\caption{A pointcloud representation of the stairs in Figure \ref{stereo-image-pair}.}
\label{pointcloud-example}
\end{figure}

\subsection{Planar Decomposition of Point Clouds} \label{plane-section}

In order to estimate a polytope representation of an obstacle, decompose its point cloud into a set of planes. We demonstrate two main approaches for finding planar surfaces of an obstacle. We describe the Contour Method in Section \ref{contour-method} and the Orientation Method in \ref{orient-method}.

Each method has its advantages and limitations. While the Contour Method can generalize to planar surfaces with arbitary orientation, it is highly sensitive to the texture and edges of obstacles. The Orientation Method is more robust to appearance, but is limited to vertical and horizontal planes.

In both approaches, MSAC (M-Estimator SAmple Consensus \cite{msac-article}) is the algorithm used for plane estimation. In \cite{msac-article}, Zisserman et. al. show that MSAC is more robust than RANSAC for estimating geometric relationships in point cloud data, at no additional computational cost.

\subsubsection{Contour Method} \label{contour-method}

As the name of this approach suggests, we detect large contours in the original RGB image and project these contours into the point cloud to find subsets points that are likely to be coplanar.

We use MATLAB's \textit{imcontour} function from the Computer Vision Toolkit to efficiently find contours. Internally, this function uses the Theo Pavlidis algorithm \cite{pavlidis} to find contours at a variable number of levels.

Contours are then sorted by area, and pairs of contours with Intersection-over-Union (IoU) above a threshold of 0.3 are replaced by the larger of the two contours. Highly overlapping contours are likely to lie on the same plane, and we observe empirically that our reconstruction performs better when there are no duplicate planes.

We then project our candidate contours into the point cloud, and find 3D points that originally from inside of this contour in the RGB image. A plane is fit to each planar subset of the point cloud using MSAC.

\begin{figure}[!h]
\centering
\includegraphics[width=2.5in]{Sections/Figures/good_contour_plot_12-7.png}
\caption{An example contour plot of the stairs in Figure \ref{stereo-anaglyph}. Here we have shown the eight largest contours in the image by area.}
\label{contours-example}
\end{figure}

\subsubsection{Orientation Method} \label{orient-method}

The Contour Method, as one might expect, is highly sensitive to lighting effects and the sharpness of edges on the obstacle. In this section, we propose a second method that relies on \textit{au priori} knowledge of plane orientation to remove our dependence on image contours.

In this method, we assume that the obstacle is composed of a predetermined number of planes that are either horizontal and vertical relative to the world frame. While performing MSAC, we sample three points from the point cloud and compute the parameters of a plane with them. We require that the normal of this plane is within an angular tolerance of a reference normal vector (i.e the z-axis for vertical planes and the y-axis for horizontal planes). If the robot frame is not aligned with the world frame, we can rotate our reference normals before performing MSAC. After performing a fixed number of iterations, MSAC selects a horizontal or vertical plane with the lowest cost. We then remove the inlier points from the point cloud, and iteratively fit horizontal and vertical planes to the residual point cloud.

Although our algorithm currently looks for a fixed number of planes, it would be straightforward to add a condition that stops extracting planes once the residual point cloud is too small, or the cost of MSAC planes is above a threshold.

\subsection{Polytope Reconstruction from Unordered Planes} \label{polytope-section}

At this stage in the pipeline, we have an unordered set of planes, each represented by four parameters: a surface normal and offset constant. There are many ways that these planes could intersect to form a polytope, but only one that actually matches our ground truth observation. We must make another assumption here about the composition of the polytope.

When using the Contour Method in Section \ref{contour-method}, we assume that the depth and height ordering of planes in 3D space is given by the vertical ordering of planes in the RGB image. Horizontal planes that appear higher in the RGB image are also higher in the scene. Similarly, vertical planes that appear higher in the RGB image are deeper in the scene.

\begin{figure}[!h]
\centering
\includegraphics[width=2.5in]{Sections/Figures/polytope_example.png}
\caption{A polytope reconstruction of the image in Figure \ref{stereo-image-pair}, using the Contour Method (See Section \ref{contour-method}). The pink, red, and blue planes correspond to horizontal faces of the stairs. The light-blue, green, and yellow planes are vertical faces. While the point cloud for these stairs contains about 245,000 points, this polytope is represented by only four parameters per plane.}
\label{polytope-diagonal-contour}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=2.5in]{Sections/Figures/polytope_sideview.png}
\caption{The polytope from Figure \ref{polytope-diagonal-contour} viewed from the side. The vertical faces of this polytope (light-blue, green, yellow) are very close to the groundtruth dimension of 20cm. Similarly, the remaining horizontal faces are close to their groundtruth dimension of 31cm.}
\label{polytope-sideview-contour}
\end{figure}

When using the Orientation Method in Section \ref{orient-method}, we use a sampling based approach to order planes. First, we sort all vertical planes based on their distance from the camera, then find the horizontal plane that is ``active'' between each pair of vertical planes. The ``active'' horizontal plane is the one that has the smallest average distance to a sample of points in the region between the vertical planes.


\begin{figure}[!h]
\centering
\includegraphics[width=3.3in]{Sections/Figures/polytope_diagonal_geom.jpg}
\caption{A polytope reconstruction of the image in Figure \ref{stereo-image-pair}, using the Orientation Method (See Section \ref{orient-method}). Here, the pink, red, and blue planes are vertical, and the light-blue, green, and yellow are horizontal. Because the orientation method is not limited by image contours, we are able to construct a polytope with more faces than in Figure \ref{polytope-diagonal-contour}}.
\label{polytope-diagonal-orient}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=2.5in]{Sections/Figures/geom_sideview_2d.jpg}
\caption{The polytope from Figure \ref{polytope-diagonal-orient} viewed from the side. Again, we see that vertical and horizontal planes are close to the groundtruth dimensions.}
\label{polytope-sideview-orient}
\end{figure}

Finally, once an ordering of planes has been established, we can compute lines of intersection between these planes, and obtain a set of vertices for the polytope. See Figures \ref{polytope-diagonal-contour} and \ref{polytope-diagonal-orient} for example reconstructions. See Figures \ref{polytope-sideview-contour} and \ref{polytope-sideview-orient} for a metric comparison of our reconstructions to groundtruth.
